# Selenium End-to-End Testing Guidelines

*All web-based interfaces MUST include Selenium testing to emulate actual user viewing and interaction patterns.*

## üéØ Core Principle

**Every web application requires comprehensive E2E testing with Selenium to validate real user workflows and interface behavior.**

### **Scope: ALL Browser-Based Interfaces**

Selenium E2E testing is MANDATORY for:
- ‚úÖ Production user-facing websites and web applications
- ‚úÖ Internal tools and dashboards
- ‚úÖ Admin panels and management interfaces
- ‚úÖ Prototypes and proof-of-concept applications
- ‚úÖ Developer tools with web interfaces
- ‚úÖ API documentation pages (if interactive)
- ‚úÖ Configuration interfaces
- ‚úÖ Monitoring dashboards

**If it runs in a browser, it MUST have Selenium tests.**

This ensures:
- Visual rendering works correctly across browsers
- Interactive elements respond properly
- Workflows complete successfully
- Error handling works as expected
- Responsive design functions properly

## üö® EVIDENCE REQUIREMENTS (NO EVIDENCE = NO COMPLETION)

**ABSOLUTE BLOCKER: Cannot mark UI testing complete without ALL evidence files.**

### **Mandatory Evidence Files (Minimum 6 Required)**

For EVERY UI feature or change, you MUST create and provide:

1. **`test_[FEATURE]_selenium.py`** - The actual Selenium test script
   - Must contain real browser automation code
   - Must interact with actual UI elements (clicks, form fills, navigation)
   - Must include assertions that verify expected behavior
   - Must NOT be mock code or placeholder code

2. **`[FEATURE]_selenium_results.txt`** - Test execution output
   - Must show actual execution results (passed/failed tests)
   - Must include test counts (e.g., "8/8 tests passed")
   - Must include timestamps showing when tests ran
   - Must include any error messages if tests failed

3. **`screenshots/[FEATURE]_chrome_*.png`** - Chrome browser screenshots
   - Must show actual application pages during test execution
   - Must capture key states (before action, after action, success states)
   - Minimum 3 screenshots per feature
   - Must be actual screenshots, not descriptions

4. **`screenshots/[FEATURE]_firefox_*.png`** - Firefox browser screenshots
   - Same requirements as Chrome screenshots
   - Must demonstrate cross-browser consistency
   - Must show same test scenarios as Chrome

5. **`console_errors_[FEATURE].txt`** - Browser console logs
   - **MUST show 0 SEVERE errors** (non-negotiable)
   - Must include full console output from browser
   - Must be captured during actual test execution
   - Format: "‚úÖ PASSED: 0 console errors" or "‚ùå FAILED: X console errors found"

6. **`[FEATURE]_test_evidence.md`** - Completed testing-evidence-template.md
   - Must document all 10 testing protocol steps
   - Must reference all other evidence files
   - Must include database verification (if applicable)
   - Must include authentication verification (if applicable)

### **Enforcement Rules**

- ‚úÖ **With 6+ evidence files** ‚Üí Task can be marked complete
- ‚ùå **Without evidence files** ‚Üí Task INCOMPLETE - no exceptions
- ‚ùå **With placeholder/mock evidence** ‚Üí Task INCOMPLETE - must redo with real evidence
- ‚ùå **With evidence from only 1 browser** ‚Üí Task INCOMPLETE - need Chrome AND Firefox
- ‚ùå **With console errors present** ‚Üí Task FAILED - must fix errors and retest

### **Before/After Comparison for Evidence Validation**

**‚ùå INVALID (No Evidence):**
```
"UI testing completed successfully"
"Selenium tests pass"
"Tested and verified working"
```

**‚úÖ VALID (With Evidence):**
```
"Selenium testing complete:
- test_pricing_update_selenium.py created (127 lines, 8 test methods)
- pricing_selenium_results.txt: 8/8 tests passed in 45.3 seconds
- screenshots/pricing_chrome_before.png, screenshots/pricing_chrome_after.png (Chrome v120)
- screenshots/pricing_firefox_before.png, screenshots/pricing_firefox_after.png (Firefox v121)
- console_errors_pricing.txt: 0 SEVERE errors
- pricing_test_evidence.md: All 10 protocol steps documented
Total: 10 evidence files created"
```

### **Template Compliance Verification**

Each evidence file must use the templates provided in this document:
- Selenium test scripts ‚Üí Use BasePage pattern (lines 286-345)
- Console error checking ‚Üí Use check_console_errors() template (see section below)
- Multi-browser testing ‚Üí Use browser matrix configuration (lines 456-490)
- Evidence documentation ‚Üí Use testing-evidence-template.md

**Failure to use templates = Evidence invalid = Task incomplete**

## üìã Selenium Testing Requirements

### **Mandatory E2E Test Coverage**
- [ ] **User Authentication Flow** - Login, logout, session management
- [ ] **Core User Journeys** - Primary application workflows
- [ ] **Form Validation** - Client-side and server-side validation
- [ ] **Navigation** - Menu systems, routing, deep linking
- [ ] **Responsive Design** - Mobile, tablet, desktop viewports
- [ ] **Cross-Browser Compatibility** - Chrome, Firefox, Safari, Edge
- [ ] **Error Handling** - 404s, 500s, network failures
- [ ] **Data CRUD Operations** - Create, Read, Update, Delete workflows

## üö® CRITICAL: Authentication Testing Rules

**‚ö†Ô∏è ABSOLUTE REQUIREMENT: Never Bypass Authentication in E2E Tests**

### **‚ùå PROHIBITED ANTIPATTERNS:**

These approaches are **NEVER ACCEPTABLE** in E2E testing:

- ‚ùå Creating "infrastructure tests" that bypass login
- ‚ùå Testing protected APIs without authenticated sessions
- ‚ùå Claiming "authentication not needed for verification"
- ‚ùå Any form of login workaround or bypass
- ‚ùå Creating separate "focused tests" to avoid authentication issues
- ‚ùå Mocking authentication in E2E tests
- ‚ùå Using admin backdoors or special test endpoints
- ‚ùå Setting session cookies manually to skip login
- ‚ùå Directly accessing protected URLs without login
- ‚ùå Testing "just the backend" without full authentication flow

**‚ö†Ô∏è Why These Are Prohibited:**
- Hides authentication bugs that real users would encounter
- Creates false sense of security about application functionality
- Misses session management issues
- Bypasses critical security checks
- Produces invalid test results

### **‚úÖ REQUIRED AUTHENTICATION TESTING PATTERN:**

**Every E2E test MUST follow this pattern:**

```python
def test_feature_with_authentication(self):
    """
    CORRECT: Full authentication flow before testing feature
    """
    # Step 1: Navigate to login page
    self.driver.get(f"{BASE_URL}/login")

    # Step 2: Enter credentials from environment variables (never hardcoded)
    username_field = self.wait_for_element((By.NAME, "username"))
    password_field = self.driver.find_element(By.NAME, "password")

    test_user = os.getenv("TEST_USER_EMAIL")  # From .env
    test_password = os.getenv("TEST_USER_PASSWORD")  # From .env

    username_field.send_keys(test_user)
    password_field.send_keys(test_password)

    # Step 3: Submit login form
    password_field.submit()

    # Step 4: Wait for redirect to authenticated page
    self.wait.until(lambda d: "/dashboard" in d.current_url)

    # Step 5: Verify session established
    assert "session_id" in [cookie['name'] for cookie in self.driver.get_cookies()]

    # Step 6: NOW test the actual feature (in authenticated context)
    self.driver.get(f"{BASE_URL}/protected/feature")

    # Test feature functionality here...
    # All actions occur as authenticated user
```

### **Failure Criteria (NO EXCEPTIONS):**
- **If login fails** ‚Üí Test failed ‚Üí Task incomplete ‚Üí FIX THE LOGIN
- **If session not established** ‚Üí Test failed ‚Üí Task incomplete ‚Üí FIX AUTHENTICATION
- **If any bypass used** ‚Üí Test invalid ‚Üí Task incomplete ‚Üí REWRITE TEST
- **If authentication broken** ‚Üí DO NOT WORK AROUND ‚Üí FIX THE AUTH SYSTEM

**Remember: Authentication shortcuts hide critical bugs and security vulnerabilities.**

## üóÉÔ∏è Database Verification in E2E Tests

**‚ö†Ô∏è CRITICAL: UI Success ‚â† Backend Success**

### **Mandatory Database Verification Pattern:**

For any E2E test that modifies data, **MUST verify database changes**:

```python
def test_data_modification_with_db_verification(self):
    """
    CORRECT: Verify database changes, not just UI feedback
    """
    # Authenticate first (always required)
    self.login_as_test_user()

    # BEFORE: Record current database state
    before_value = self.query_database(
        "SELECT status FROM orders WHERE id = 12345"
    )
    assert before_value == "pending"  # Confirm starting state

    # Execute UI action
    self.driver.get(f"{BASE_URL}/orders/12345")
    approve_button = self.wait_for_element((By.ID, "approve-order"))
    approve_button.click()

    # Wait for UI feedback
    success_message = self.wait_for_element((By.CLASS_NAME, "success-message"))
    assert "Order approved" in success_message.text

    # AFTER: Verify database actually changed
    after_value = self.query_database(
        "SELECT status FROM orders WHERE id = 12345"
    )
    assert after_value == "approved"  # CRITICAL: Database must change

    # Document in test evidence
    self.log_evidence({
        "before": before_value,
        "after": after_value,
        "ui_feedback": success_message.text,
        "verdict": "PASS - Database updated correctly"
    })
```

### **Database Verification Helper Methods:**

```python
import pyodbc  # or psycopg2, mysql.connector, etc.

class E2ETestBase:
    def query_database(self, sql_query):
        """Execute SQL query and return result"""
        conn_string = os.getenv("TEST_DATABASE_URL")
        with pyodbc.connect(conn_string) as conn:
            cursor = conn.cursor()
            cursor.execute(sql_query)
            result = cursor.fetchone()
            return result[0] if result else None

    def verify_database_change(self, query, expected_before, expected_after):
        """
        Verify database value changes from before to after
        Returns: (success: bool, actual_before, actual_after)
        """
        # This helper would be called before and after UI action
        pass
```

### **Critical Verification Rules:**
- ‚úÖ **ALWAYS query database before and after UI actions**
- ‚úÖ **Compare actual values, not just check for "not null"**
- ‚úÖ **Document database queries in test evidence**
- ‚úÖ **Fail test if database unchanged when change expected**

### **Failure Scenarios:**
- ‚ùå **UI shows success but database unchanged** ‚Üí Test FAILED
- ‚ùå **Database changed but different than expected** ‚Üí Test FAILED
- ‚ùå **Cannot connect to database to verify** ‚Üí Fix environment, rerun test
- ‚ùå **No database verification for data-modifying test** ‚Üí Test INCOMPLETE

## üõ† Selenium Grid Setup

### **Docker Selenium Grid**
```yaml
# docker-compose.selenium.yml
version: '3.8'
services:
  selenium-hub:
    image: selenium/hub:4.15.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    environment:
      - GRID_MAX_SESSION=16
      - GRID_BROWSER_TIMEOUT=300
      - GRID_TIMEOUT=300

  chrome:
    image: selenium/node-chrome:4.15.0
    shm_size: 2gb
    depends_on:
      - selenium-hub
    environment:
      - HUB_HOST=selenium-hub
      - NODE_MAX_INSTANCES=4
      - NODE_MAX_SESSION=4
    scale: 2

  firefox:
    image: selenium/node-firefox:4.15.0
    shm_size: 2gb
    depends_on:
      - selenium-hub
    environment:
      - HUB_HOST=selenium-hub
      - NODE_MAX_INSTANCES=4
      - NODE_MAX_SESSION=4
    scale: 2

  edge:
    image: selenium/node-edge:4.15.0
    shm_size: 2gb
    depends_on:
      - selenium-hub
    environment:
      - HUB_HOST=selenium-hub
      - NODE_MAX_INSTANCES=4
      - NODE_MAX_SESSION=4
```

### **Grid Management Commands**
```bash
# Start Selenium Grid
docker-compose -f docker-compose.selenium.yml up -d

# Scale browser nodes
docker-compose -f docker-compose.selenium.yml up -d --scale chrome=4 --scale firefox=2

# Monitor grid status
curl http://localhost:4444/wd/hub/status

# Stop grid
docker-compose -f docker-compose.selenium.yml down
```

## üß™ Test Framework Structure

### **Page Object Model Implementation**
```
tests/
‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_page.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login_page.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard_page.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_management_page.py
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_data.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_credentials.json
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ driver_factory.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wait_helpers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ screenshot_helper.py
‚îÇ   ‚îî‚îÄ‚îÄ test_suites/
‚îÇ       ‚îú‚îÄ‚îÄ test_authentication.py
‚îÇ       ‚îú‚îÄ‚îÄ test_user_workflows.py
‚îÇ       ‚îî‚îÄ‚îÄ test_responsive_design.py
```

### **Base Page Class**
```python
# pages/base_page.py
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
import logging

class BasePage:
    def __init__(self, driver):
        self.driver = driver
        self.wait = WebDriverWait(driver, 10)
        self.logger = logging.getLogger(__name__)

    def wait_for_element(self, locator, timeout=10):
        """Wait for element to be present and visible"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.visibility_of_element_located(locator)
            )
            return element
        except TimeoutException:
            self.logger.error(f"Element not found: {locator}")
            self.take_screenshot("element_not_found")
            raise

    def wait_for_clickable(self, locator, timeout=10):
        """Wait for element to be clickable"""
        return WebDriverWait(self.driver, timeout).until(
            EC.element_to_be_clickable(locator)
        )

    def safe_click(self, locator):
        """Click element with wait and error handling"""
        element = self.wait_for_clickable(locator)
        self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
        element.click()

    def safe_send_keys(self, locator, text):
        """Send keys with wait and clear"""
        element = self.wait_for_element(locator)
        element.clear()
        element.send_keys(text)

    def get_text(self, locator):
        """Get element text with wait"""
        element = self.wait_for_element(locator)
        return element.text

    def take_screenshot(self, name):
        """Take screenshot for debugging"""
        filename = f"screenshots/{name}_{self.driver.current_url.split('/')[-1]}.png"
        self.driver.save_screenshot(filename)
        self.logger.info(f"Screenshot saved: {filename}")

    def scroll_to_element(self, locator):
        """Scroll element into view"""
        element = self.wait_for_element(locator)
        self.driver.execute_script("arguments[0].scrollIntoView(true);", element)

    def check_console_errors(self, feature_name):
        """
        üö® MANDATORY: Check browser console for errors

        This method MUST be called in every UI test to verify 0 console errors.
        Creates console_errors_[FEATURE].txt evidence file.

        Args:
            feature_name: Name of the feature being tested (for filename)

        Raises:
            AssertionError: If any SEVERE console errors are found

        Returns:
            int: Number of severe errors found (should always be 0)
        """
        import os

        # Get browser console logs
        logs = self.driver.get_log('browser')

        # Filter for SEVERE errors (warnings are acceptable in some cases)
        severe_errors = [log for log in logs if log['level'] == 'SEVERE']

        # Create evidence file
        evidence_dir = 'evidence'
        os.makedirs(evidence_dir, exist_ok=True)
        evidence_file = os.path.join(evidence_dir, f'console_errors_{feature_name}.txt')

        with open(evidence_file, 'w', encoding='utf-8') as f:
            f.write(f"Browser Console Error Check - {feature_name}\n")
            f.write(f"{'=' * 60}\n\n")
            f.write(f"Test Timestamp: {self._get_timestamp()}\n")
            f.write(f"Browser: {self.driver.capabilities.get('browserName', 'unknown')}\n")
            f.write(f"Total Log Entries: {len(logs)}\n")
            f.write(f"SEVERE Errors: {len(severe_errors)}\n\n")

            if severe_errors:
                f.write("‚ùå FAILED: Console errors detected\n\n")
                f.write("SEVERE Errors:\n")
                f.write("-" * 60 + "\n")
                for idx, error in enumerate(severe_errors, 1):
                    f.write(f"\nError #{idx}:\n")
                    f.write(f"  Level: {error['level']}\n")
                    f.write(f"  Message: {error['message']}\n")
                    f.write(f"  Source: {error.get('source', 'unknown')}\n")
                    f.write(f"  Timestamp: {error['timestamp']}\n")

                # FAIL THE TEST
                self.logger.error(f"Console errors detected: {len(severe_errors)}")
                raise AssertionError(
                    f"‚ùå FAILED: {len(severe_errors)} SEVERE console errors found. "
                    f"See {evidence_file} for details."
                )
            else:
                f.write("‚úÖ PASSED: 0 SEVERE console errors\n\n")
                f.write("All log entries:\n")
                f.write("-" * 60 + "\n")
                for log in logs:
                    f.write(f"[{log['level']}] {log['message']}\n")

        self.logger.info(f"Console error check complete: {evidence_file}")
        return len(severe_errors)

    def _get_timestamp(self):
        """Get current timestamp for evidence files"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
```

### **Complete E2E Test Example with Evidence Collection**

```python
# tests/e2e/test_pricing_update_feature.py
import unittest
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from pages.base_page import BasePage
from pages.login_page import LoginPage
from pages.pricing_page import PricingPage
import pyodbc

class TestPricingUpdateFeature(unittest.TestCase):
    """
    Complete E2E test with all evidence collection requirements
    Demonstrates proper authentication, database verification, and evidence creation
    """

    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        cls.driver = webdriver.Chrome()
        cls.driver.maximize_window()
        cls.base_url = os.getenv('TEST_BASE_URL', 'http://localhost:5000')
        cls.feature_name = 'pricing_update'

        # Create evidence directories
        os.makedirs('screenshots', exist_ok=True)
        os.makedirs('evidence', exist_ok=True)

    def test_01_pricing_update_workflow(self):
        """Test complete pricing update workflow with database verification"""

        # STEP 1: AUTHENTICATION (MANDATORY - NO BYPASS)
        login_page = LoginPage(self.driver)
        login_page.navigate_to_login()
        login_page.login(
            os.getenv('TEST_USER_EMAIL'),
            os.getenv('TEST_USER_PASSWORD')
        )

        # Verify authentication successful
        self.assertIn('/dashboard', self.driver.current_url)
        self.driver.save_screenshot('screenshots/pricing_chrome_01_authenticated.png')

        # STEP 2: DATABASE VERIFICATION - BEFORE
        invoice_id = 12345
        part_number = 'ABC123'

        before_price = self._query_database(
            f"SELECT UnitCost FROM InvoiceItems WHERE InvoiceId={invoice_id} AND PartNumber='{part_number}'"
        )
        self.assertIsNotNone(before_price, "Test data must exist")
        print(f"BEFORE: UnitCost = {before_price}")

        # STEP 3: NAVIGATE TO PRICING PAGE
        pricing_page = PricingPage(self.driver)
        pricing_page.navigate_to_invoice(invoice_id)
        self.driver.save_screenshot('screenshots/pricing_chrome_02_invoice_loaded.png')

        # STEP 4: EXECUTE UI ACTIONS
        pricing_page.click_part_row(part_number)
        pricing_page.update_unit_cost(12.75)
        pricing_page.click_save_button()

        # Wait for success confirmation
        success_msg = pricing_page.wait_for_success_message()
        self.assertIn('updated', success_msg.lower())
        self.driver.save_screenshot('screenshots/pricing_chrome_03_update_success.png')

        # STEP 5: DATABASE VERIFICATION - AFTER
        after_price = self._query_database(
            f"SELECT UnitCost FROM InvoiceItems WHERE InvoiceId={invoice_id} AND PartNumber='{part_number}'"
        )
        print(f"AFTER: UnitCost = {after_price}")

        # CRITICAL VERIFICATION
        self.assertEqual(after_price, 12.75,
            "‚ùå FAILED: Database not updated - UI shows success but backend did not change")

        # STEP 6: CONSOLE ERROR CHECK (MANDATORY)
        base_page = BasePage(self.driver)
        errors = base_page.check_console_errors(self.feature_name)
        self.assertEqual(errors, 0, "Console errors detected")

        # STEP 7: DOCUMENT EVIDENCE
        self._write_test_results({
            'invoice_id': invoice_id,
            'part_number': part_number,
            'before_price': before_price,
            'after_price': after_price,
            'expected_price': 12.75,
            'database_updated': after_price == 12.75,
            'console_errors': errors
        })

    def _query_database(self, sql):
        """Execute SQL query and return result"""
        conn_string = os.getenv('TEST_DATABASE_URL')
        with pyodbc.connect(conn_string) as conn:
            cursor = conn.cursor()
            cursor.execute(sql)
            result = cursor.fetchone()
            return result[0] if result else None

    def _write_test_results(self, results):
        """Write test results to evidence file"""
        with open(f'evidence/{self.feature_name}_selenium_results.txt', 'w') as f:
            f.write(f"Selenium Test Results - {self.feature_name}\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Test: test_01_pricing_update_workflow\n")
            f.write(f"Status: ‚úÖ PASSED\n\n")
            f.write("Database Verification:\n")
            f.write(f"  Invoice ID: {results['invoice_id']}\n")
            f.write(f"  Part Number: {results['part_number']}\n")
            f.write(f"  Before: ${results['before_price']}\n")
            f.write(f"  After: ${results['after_price']}\n")
            f.write(f"  Expected: ${results['expected_price']}\n")
            f.write(f"  Database Updated: {'‚úÖ YES' if results['database_updated'] else '‚ùå NO'}\n\n")
            f.write(f"Console Errors: {results['console_errors']}\n")

    @classmethod
    def tearDownClass(cls):
        """Clean up"""
        cls.driver.quit()

if __name__ == '__main__':
    unittest.main()
```

### **Driver Factory**
```python
# utils/driver_factory.py
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.edge.options import Options as EdgeOptions
import os

class DriverFactory:
    @staticmethod
    def create_driver(browser="chrome", headless=False, grid_url=None):
        """Create WebDriver instance"""

        if grid_url:
            return DriverFactory._create_remote_driver(browser, headless, grid_url)
        else:
            return DriverFactory._create_local_driver(browser, headless)

    @staticmethod
    def _create_remote_driver(browser, headless, grid_url):
        """Create remote WebDriver for Selenium Grid"""
        options = DriverFactory._get_browser_options(browser, headless)

        return webdriver.Remote(
            command_executor=grid_url,
            options=options
        )

    @staticmethod
    def _create_local_driver(browser, headless):
        """Create local WebDriver"""
        if browser.lower() == "chrome":
            options = ChromeOptions()
            if headless:
                options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-infobars")
            options.add_argument("--disable-popup-blocking")
            options.add_argument("--disable-notifications")

            # Set clipboard and popup preferences
            prefs = {
                "profile.content_settings.exceptions.clipboard": {
                    "*": {"setting": 1}
                },
                "profile.default_content_settings.popups": 0
            }
            options.add_experimental_option("prefs", prefs)

            return webdriver.Chrome(options=options)

        elif browser.lower() == "firefox":
            options = FirefoxOptions()
            if headless:
                options.add_argument("--headless")
            return webdriver.Firefox(options=options)

        elif browser.lower() == "edge":
            options = EdgeOptions()
            if headless:
                options.add_argument("--headless")
            return webdriver.Edge(options=options)

        else:
            raise ValueError(f"Unsupported browser: {browser}")

    @staticmethod
    def _get_browser_options(browser, headless):
        """Get browser-specific options"""
        if browser.lower() == "chrome":
            options = ChromeOptions()
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-infobars")
            options.add_argument("--disable-popup-blocking")
            options.add_argument("--disable-notifications")

            # Set clipboard and popup preferences
            prefs = {
                "profile.content_settings.exceptions.clipboard": {
                    "*": {"setting": 1}
                },
                "profile.default_content_settings.popups": 0
            }
            options.add_experimental_option("prefs", prefs)
        elif browser.lower() == "firefox":
            options = FirefoxOptions()
            options.add_argument("--width=1920")
            options.add_argument("--height=1080")
        elif browser.lower() == "edge":
            options = EdgeOptions()
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
        else:
            raise ValueError(f"Unsupported browser: {browser}")

        if headless:
            options.add_argument("--headless")

        return options
```

## üé≠ Cross-Browser Testing Strategy

### **Browser Matrix Configuration**
```python
# conftest.py for pytest
import pytest
from utils.driver_factory import DriverFactory

BROWSERS = ["chrome", "firefox", "edge"]
VIEWPORTS = [
    (1920, 1080),  # Desktop
    (1366, 768),   # Laptop
    (768, 1024),   # Tablet
    (375, 667)     # Mobile
]

@pytest.fixture(params=BROWSERS)
def browser_driver(request):
    """Parameterized browser fixture"""
    browser = request.param
    grid_url = "http://localhost:4444/wd/hub"

    driver = DriverFactory.create_driver(
        browser=browser,
        headless=True,
        grid_url=grid_url
    )

    yield driver
    driver.quit()

@pytest.fixture(params=VIEWPORTS)
def viewport_driver(browser_driver, request):
    """Parameterized viewport fixture"""
    width, height = request.param
    browser_driver.set_window_size(width, height)
    return browser_driver
```

## üì± Responsive Design Testing

### **Viewport Testing Implementation**
```python
# test_responsive_design.py
import pytest
from pages.dashboard_page import DashboardPage

class TestResponsiveDesign:

    @pytest.mark.parametrize("viewport", [
        (1920, 1080, "desktop"),
        (1366, 768, "laptop"),
        (768, 1024, "tablet"),
        (375, 667, "mobile")
    ])
    def test_navigation_responsive(self, browser_driver, viewport):
        """Test navigation adapts to different viewports"""
        width, height, device_type = viewport
        browser_driver.set_window_size(width, height)

        dashboard = DashboardPage(browser_driver)
        dashboard.navigate_to_dashboard()

        # Verify navigation elements based on viewport
        if device_type in ["mobile", "tablet"]:
            assert dashboard.is_hamburger_menu_visible()
            dashboard.click_hamburger_menu()
            assert dashboard.is_mobile_menu_expanded()
        else:
            assert dashboard.is_desktop_navigation_visible()
            assert not dashboard.is_hamburger_menu_visible()

    def test_form_layouts_responsive(self, viewport_driver):
        """Test form layouts across viewports"""
        dashboard = DashboardPage(viewport_driver)
        dashboard.navigate_to_user_form()

        # Verify form adapts properly
        assert dashboard.is_form_visible()
        assert dashboard.are_form_fields_accessible()

        # Test form submission
        dashboard.fill_user_form({
            "name": "Test User",
            "email": "test@example.com"
        })
        dashboard.submit_form()
        assert dashboard.is_success_message_visible()
```

## üîÑ CI/CD Integration

### **GitHub Actions Selenium Pipeline**
```yaml
# .github/workflows/e2e-tests.yml
name: E2E Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  e2e-tests:
    runs-on: ubuntu-latest

    services:
      selenium-hub:
        image: selenium/hub:4.15.0
        ports:
          - 4444:4444

      chrome:
        image: selenium/node-chrome:4.15.0
        env:
          HUB_HOST: selenium-hub

    steps:
    - uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements-test.txt

    - name: Wait for Selenium Grid
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:4444/wd/hub/status; do sleep 2; done'

    - name: Run E2E Tests
      run: |
        pytest tests/e2e/ --html=reports/e2e-report.html --self-contained-html
      env:
        SELENIUM_GRID_URL: http://localhost:4444/wd/hub
        BASE_URL: http://localhost:3000

    - name: Upload test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-reports
        path: |
          reports/
          screenshots/
```

## üö® Selenium Testing Protocol

### **Pre-Development Requirements**
- [ ] **Selenium Grid configured** - Docker containers running
- [ ] **Test data prepared** - Fixtures and mock data ready
- [ ] **Page objects defined** - All pages have corresponding classes
- [ ] **Test scenarios documented** - User stories translated to test cases

### **During Development**
- [ ] **Write E2E tests alongside features** - Test-driven development
- [ ] **Validate across browsers** - Chrome, Firefox, Edge minimum
- [ ] **Test responsive behavior** - Mobile, tablet, desktop viewports
- [ ] **Verify accessibility** - Screen reader compatibility, keyboard navigation

### **Pre-Deployment Validation**
- [ ] **Full E2E test suite passes** - All critical user journeys work
- [ ] **Cross-browser compatibility confirmed** - No browser-specific issues
- [ ] **Performance acceptable** - Page load times under thresholds
- [ ] **Error scenarios handled** - Graceful degradation tested

## üìä Reporting and Monitoring

### **Test Reporting Configuration**
```python
# pytest.ini
[tool:pytest]
addopts =
    --html=reports/selenium-report.html
    --self-contained-html
    --capture=tee-sys
    --tb=short
    -v
testpaths = tests/e2e
python_files = test_*.py
python_classes = Test*
python_functions = test_*
markers =
    smoke: Quick smoke tests
    regression: Full regression suite
    cross_browser: Cross-browser tests
```

### **Monitoring Integration**
```python
# utils/test_monitor.py
import requests
import json
from datetime import datetime

class TestMonitor:
    def __init__(self, webhook_url=None):
        self.webhook_url = webhook_url

    def report_test_results(self, results):
        """Send test results to monitoring system"""
        payload = {
            "timestamp": datetime.utcnow().isoformat(),
            "test_suite": "e2e_selenium",
            "total_tests": results.total,
            "passed": results.passed,
            "failed": results.failed,
            "skipped": results.skipped,
            "duration": results.duration
        }

        if self.webhook_url:
            requests.post(self.webhook_url, json=payload)
```

---

*This document ensures comprehensive E2E testing coverage for all web-based interfaces using Selenium automation.*